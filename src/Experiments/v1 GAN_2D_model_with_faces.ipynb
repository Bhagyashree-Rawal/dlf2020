{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_2D_model_with_faces.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZaYyfFUqUnGY"},"source":["# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5LM2NUrWCxz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606007579002,"user_tz":480,"elapsed":18832,"user":{"displayName":"Dhvani Kansara","photoUrl":"https://lh6.googleusercontent.com/--vEPG3UzR7c/AAAAAAAAAAI/AAAAAAAAA3M/g_iJkgIHykk/s64/photo.jpg","userId":"11007117951358398147"}},"outputId":"fedcaf1e-26fc-42bb-8d95-6f2753814d97"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/DLF2020/dlf2020'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1XCqSJ4wOA_0YGBeiVFwWnOq2NUCpBEMO/DLF2020/dlf2020\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4LTc4HW7UgQI"},"source":["# Standard PyTorch imports\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math, copy\n","from torch.autograd import Variable\n","\n","# For plots\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","import json\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","import pickle\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# from torchtext.vocab import GloVe\n","# glove_embedding = GloVe(dim = 100)\n","# print(torch.cat((glove_embedding['apart'], torch.zeros(2))))\n","# print(glove_embedding['apart'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ies1PnOmrKbK"},"source":["embeddings_dict = {}\n","with open(\"data/interim/glove.6B.200d.txt\", 'r') as f: \n","      for line in f:\n","          values = line.split()\n","          word = values[0]\n","          vector = np.asarray(values[1:], \"float32\")\n","          embeddings_dict[word] = vector\n","with open(\"data/interim/glove.6B.50d.txt\", 'r') as f: \n","      for line in f:\n","          values = line.split()\n","          word = values[0]\n","          vector = np.asarray(values[1:], \"float32\")\n","          embeddings_dict[word] = np.append(embeddings_dict[word],vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5AGZh8HRVp_"},"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    device = 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AC8KeDJUgQO"},"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base model for this and many \n","    other models.\n","    \"\"\"\n","    def __init__(self, encoder, decoder):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def forward(self, src, tgt, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        memory = self.encoder(src)\n","        output = self.decoder(tgt, memory, tgt_mask)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yy7pY85UgQR"},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psiq5idJUgQT"},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N = 1):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        \n","    def forward(self, x):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEz9kLClUgQV"},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zx9JBwAcUgQY"},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity we apply the norm first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer function that maintains the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mEBw9tIUgQb"},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n","    def __init__(self, size, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.feed_forward = feed_forward\n","        self.size = size\n","\n","    def forward(self, x):\n","        \"Follow Figure 1 (left) for connections.\"\n","        return self.feed_forward(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3o_ZB42sUgQd"},"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N, continuous_embedding, counter_embedding):\n","        super(Decoder, self).__init__()\n","        self.continuous_embedding = continuous_embedding\n","        self.counter_embedding = counter_embedding\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, tgt_mask):\n","        continuous_embedding = self.continuous_embedding(x[:,:,:-counter_index:])\n","        counter_embedding = torch.cat((self.counter_embedding(x[:,:,-counter_index:-counter_index+1]),torch.zeros(x.shape[0],x.shape[1],counter_index-1).to(device)), axis=2)\n","        continuous_embedding = torch.cat((continuous_embedding,counter_embedding), axis=2)\n","        x = continuous_embedding\n","        \n","        for layer in self.layers:\n","            x = layer(x, memory, tgt_mask)\n","        return self.norm(x)\n","\n","        # with counter embedding layer = batch_size x 185\n","        # continuous_embedding = self.continuous_embedding(x[:,:,:360:])\n","        # counter_embedding = torch.cat((self.counter_embedding(x[:,:,360:361].squeeze()).unsqueeze(2),torch.zeros((x.shape[0],x.shape[1],3)).cuda()),axis=2)\n","        # input = torch.cat((continuous_embedding,counter_embedding), axis=2)\n","        # for layer in self.layers:\n","        #     input = layer(input, memory, tgt_mask)\n","        # return self.norm(input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMm6xHWVUgQg"},"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","        \n","    def forward(self, x, memory, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        # todo check src_mask can be None or should be something valid\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, None))\n","        return self.sublayer[2](x, self.feed_forward)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyQKI9AgUgQj"},"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlZ8zw9PUgQr"},"source":["def attention(query, key, value, mask=None, dropout=0.0):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    # (Dropout described below)\n","    p_attn = F.dropout(p_attn, p=dropout)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ea0UrEgUgQt"},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.p = dropout\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","                             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuDPthO2UgQx"},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model_input, d_model_output, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        # Torch linears have a `b` by default. \n","        self.w_1 = nn.Linear(d_model_input, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model_output)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVsjhp6uUgQ1"},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term) \n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dd3lP9fTUgQ9"},"source":["## Full Model"]},{"cell_type":"code","metadata":{"id":"b-LDhRoaUgQ-"},"source":["def make_model(N=6, d_model_input=102, d_model_output=102, d_ff=1024, h=5, dropout=0.1):\n","    # TRY different h values and N values remember  \n","    \"Construct a model object based on hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model_output, dropout)\n","    # ff_encoder = PositionwiseFeedForward(d_model_input, d_model_output, d_ff, dropout)\n","    # ff_encoder = nn.Sequential(nn.Linear(d_model_input, 512), nn.ReLU(), nn.Linear(512, d_model_output))\n","    ff_encoder = nn.Linear(d_model_input, d_model_output)\n","    ff_decoder = PositionwiseFeedForward(d_model_output, d_model_output, d_ff, dropout)\n","    position = PositionalEncoding(d_model_output, dropout)\n","    continuous_embedding = nn.Sequential(nn.Linear(d_model_output-counter_index, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model_output-counter_index))\n","    # counter_embedding = nn.Sequential(nn.Linear(max_sequence_length,max_sequence_length), nn.Sigmoid())\n","    counter_embedding = nn.Linear(1,1)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model_input, c(ff_encoder), dropout), N=1),\n","        Decoder(DecoderLayer(d_model_output, c(attn), c(attn), c(ff_decoder), dropout), N, continuous_embedding, counter_embedding))\n","    \n","    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qP-g4KfhUgQ_"},"source":["# Small example model.\n","# tmp_model = make_model()\n","# tmp_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MuV1e8nEUgRB"},"source":["# Training\n","\n","This section describes the training regime for our models.\n"]},{"cell_type":"code","metadata":{"id":"Q5yV0f2QUgRL"},"source":["# Note: This part is incredibly important. \n","# Need to train with this setup of the model is very unstable.\n","class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup**(-1.5)))\n","        \n","def get_std_opt(model):\n","    return NoamOpt(model.encoder.layers[0].size, 2, 4000,\n","            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGeeyOBabr_5"},"source":["pad = 2.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxd5-qdjvb1G"},"source":["import numpy\n","def normalization(Xx, Xy):\n","  T, n = Xx.shape\n","  sum0 = T * n\n","  sum1Xx = np.sum(np.sum(Xx))\n","  sum2Xx = np.sum(np.sum(Xx * Xx))\n","  sum1Xy = np.sum(np.sum(Xy))\n","  sum2Xy = np.sum(np.sum(Xy * Xy))\n","  mux = sum1Xx / sum0\n","  muy = sum1Xy / sum0\n","  sum0 = 2 * sum0\n","  sum1 = sum1Xx + sum1Xy\n","  sum2 = sum2Xx + sum2Xy\n","  mu = sum1 / sum0\n","  sigma2 = (sum2 / sum0) - mu * mu\n","  if sigma2 < 1e-10:\n","    simga2 = 1e-10\n","  sigma = math.sqrt(sigma2)\n","  return (Xx - mux) / sigma, (Xy - muy) / sigma\n","\n","def prune(Xx, Xy, Xw, watchThis, threshold, dtype):\n","  T = Xw.shape[0]\n","  N = Xw.shape[1]\n","  Yx = numpy.zeros((T, N), dtype=dtype)\n","  Yy = numpy.zeros((T, N), dtype=dtype)\n","  Yw = numpy.zeros((T, N), dtype=dtype)\n","  for t in range(T):\n","    sum0 = 0\n","    sum1 = 0.0\n","    for i in watchThis:\n","      sum0 = sum0 + 1\n","      sum1 = sum1 + Xw[t, i]\n","    Ew = sum1 / sum0\n","    if Ew >= threshold:\n","      for i in range(N):\n","        Yx[t, i] = Xx[t, i]\n","        Yy[t, i] = Xy[t, i]\n","        Yw[t, i] = Xw[t, i]\n","  return Yx, Yy, Yw\n","\n","\n","def interpolation(Xx, Xy, Xw, threshold, dtype):\n","  T = Xw.shape[0]\n","  N = Xw.shape[1]\n","  Yx = numpy.zeros((T, N), dtype=dtype)\n","  Yy = numpy.zeros((T, N), dtype=dtype)\n","  for t in range(T):\n","    for i in range(N):\n","      a1 = Xx[t, i]\n","      a2 = Xy[t, i]\n","      p = Xw[t, i]\n","      sumpa1 = p * a1\n","      sumpa2 = p * a2\n","      sump = p\n","      delta = 0\n","      while sump < threshold:\n","        change = False\n","        delta = delta + 1\n","        t2 = t + delta\n","        if t2 < T:\n","          a1 = Xx[t2, i]\n","          a2 = Xy[t2, i]\n","          p = Xw[t2, i]\n","          sumpa1 = sumpa1 + p * a1\n","          sumpa2 = sumpa2 + p * a2\n","          sump = sump + p\n","          change = True\n","        t2 = t - delta\n","        if t2 >= 0:\n","          a1 = Xx[t2, i]\n","          a2 = Xy[t2, i]\n","          p = Xw[t2, i]\n","          sumpa1 = sumpa1 + p * a1\n","          sumpa2 = sumpa2 + p * a2\n","          sump = sump + p\n","          change = True\n","        if not change:\n","          break\n","      if sump <= 0.0:\n","        sump = 1e-10\n","      Yx[t, i] = sumpa1 / sump\n","      Yy[t, i] = sumpa2 / sump\n","  return Yx, Yy, Xw\n","\n","def convList2Array(lst): \n","  T, dim = lst[0].shape\n","  a = []\n","  for t in range(T):\n","    a_t = []\n","    for i in range(dim):\n","      for j in range(len(lst)):\n","        a_t.append(lst[j][t, i])\n","    a.append(a_t)\n","    # keep output of 20 frames\n","    # downsample_rate = dim // 20\n","    # a = numpy.asarray(a)\n","    # print(dim, downsample_rate)\n","    # a = a[0:dim:downsample_rate,:]\n","  return numpy.asarray(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnB9x8JjL8BC","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"error","timestamp":1606005472783,"user_tz":480,"elapsed":693,"user":{"displayName":"Dhvani Kansara","photoUrl":"https://lh6.googleusercontent.com/--vEPG3UzR7c/AAAAAAAAAAI/AAAAAAAAA3M/g_iJkgIHykk/s64/photo.jpg","userId":"11007117951358398147"}},"outputId":"22d72f72-dfc3-4491-de46-809931c00bfc"},"source":["# the following code concatenates the 2d points from each file, we find corresponding gloss and its encoding and an array of corresponding sequences \n","# we replace z values with 0 currently\n","max_sequence_length = 0\n","pad = 2\n","sequence_limit = 175\n","counter_index = 10\n","class PoseDataset(Dataset):\n","    \"\"\"PoseDataset.\"\"\"\n","    def __init__(self, training_data_json, words_pickle, embeddings_pickle, videos_root_dir, transform=None):\n","        # we go through the entire folder containing the videos and for each video id we find the gloss and for that gloss we find the embedding from the pickle file\n","        self.embeddings = []\n","        self.poses = []\n","        f = open(training_data_json,'r')\n","        temp = json.load(f)\n","        video_gloss_mapping = {}\n","        for entry in temp:\n","            gloss = entry['gloss']\n","            for videos in entry['instances']:\n","                video_gloss_mapping[videos['video_id']] = gloss\n","        word_embeddings = []\n","        words = []\n","        # with (open(embeddings_pickle, \"rb\")) as openfile:\n","        #     while True:\n","        #         try:\n","        #             word_embeddings.append(pickle.load(openfile))\n","        #         except EOFError:\n","        #             break\n","        # with (open(words_pickle, \"rb\")) as openfile:\n","        #     while True:\n","        #         try:\n","        #             words.append(pickle.load(openfile))\n","        #         except EOFError:\n","        #             break\n","        for video_name in os.listdir(videos_root_dir):\n","          try:\n","            gloss = video_gloss_mapping[video_name[:video_name.index('.'):]]\n","            data = json.load(open(os.path.join(videos_root_dir,video_name)))\n","            max_count = max([int(i) for i in list(data.keys())])\n","            actual_count = 0\n","            temp = []\n","            # we add counter at the end of each pose\n","            # we store 0 for z positions\n","            if max_count == 0 or max_count > sequence_limit:\n","              continue\n","            for count in data.keys():\n","                # 'face_keypoints_2d', 'hand_left_keypoints_2d', hand_right_keypoints_2d','pose_keypoints_2d'\n","                # if people array is empty we need to skip that frame\n","                if len(data[count]['people']) > 0:\n","                    # added extra 0 to balance heads for multi headed attention \n","                    t = data[count]['people'][0]['pose_keypoints_2d'][:24:] + data[count]['people'][0]['hand_left_keypoints_2d'] + data[count]['people'][0]['hand_right_keypoints_2d'] + data[count]['people'][0]['face_keypoints_2d'] + [actual_count / max_count] + [0] * 9\n","                    # the following loop converts z coordinates to 0\n","                    # for i in range(2,len(t)-2,3):\n","                    #     t[i] = 0\n","                    temp.append(t)\n","                    actual_count += 1\n","            # indices = np.argwhere(words[0] == gloss)[0]\n","\n","            temp = np.array(temp)\n","            n = temp.shape[-1] - counter_index\n","            X = temp[:, 0:n:3]\n","            Y = temp[:, 1:n:3]\n","            W = temp[:, 2:n:3]\n","            # normalization\n","            X, Y = normalization(X, Y)\n","            # Delete all skeletal models which have a lot of missing parts.\n","            X, Y, W = prune(X, Y, W, (0, 1, 2, 3, 4, 5, 6, 7), 0.3, \"float32\")\n","            # Preliminary filtering: weighted linear interpolation of missing points.\n","            X, Y, W = interpolation(X, Y, W, 0.99, \"float32\")\n","            # we are removing extra points from face just to simplify dimensions\n","            arr = np.append(convList2Array([X,Y]),temp[:,-counter_index:], axis=1)\n","            # temp[:, :-2] = arr[:,:]\n","            # temp[:, 2:n:3] = 0\n","\n","        \n","            #self.embeddings.append(word_embeddings[0][indices[0]].tolist())\n","            try:\n","                self.embeddings.append(embeddings_dict[gloss])\n","                self.poses.append(arr)\n","            except:\n","                # print(gloss + \" out of vocabulary\")\n","                pass\n","          except:\n","            print('some error')\n","            pass\n","                \n","        max_rows = max([len(row) for row in self.poses])\n","        global max_sequence_length\n","        max_sequence_length = max_rows\n","        # padding to make all sequences of same length\n","        for i in range(len(self.poses)):\n","          batch = self.poses[i]\n","          padded_input = np.zeros((max_rows - len(batch), len(batch[0])))\n","          # padded_input.fill(pad)\n","          padded_input[:, -counter_index] = pad\n","          self.poses[i] = np.vstack((batch, padded_input))\n","          \n","        self.embeddings = torch.tensor(self.embeddings, dtype=torch.float32)\n","        self.poses = torch.tensor(self.poses, dtype=torch.float32)\n"," \n","        print(len(self.embeddings),len(self.embeddings[0]),len(self.poses),len(self.poses[0]), len(self.poses[0][0]))\n","        \n","    def __len__(self):\n","        return len(self.poses)\n","\n","    def __getitem__(self, idx):\n","        sample = {'embedding': self.embeddings[idx], 'poses': self.poses[idx]}\n","        return sample\n","        \n","dataset = PoseDataset(\"./Data Collection Module/WLASL_v0.3.json\", \"./data/interim/words.pkl\", \"data/interim/embeddings.pkl\", \"data/interim/Training Data/2D Data/\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-dcfcd6162522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data Collection Module/WLASL_v0.3.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/interim/words.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/interim/embeddings.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/interim/Training Data/2D Data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-dcfcd6162522>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data_json, words_pickle, embeddings_pickle, videos_root_dir, transform)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mmax_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmax_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"]}]},{"cell_type":"markdown","metadata":{"id":"OK9AqMr7wLzQ"},"source":[""]},{"cell_type":"code","metadata":{"id":"sfARuRmTPTRV"},"source":["batch_size = 20\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rw2uGjDqe1Kk"},"source":["def plot_losses(loss_list):\n","  ax1 = plt.subplot(111)\n","  ax1.plot(loss_list)\n","  ax1.title.set_text(\"Transformer losses\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSF9AaKJUgRZ"},"source":["def train_epoch(train_iter, model, criterion, opt, epoch, transpose=False):\n","    model.train()\n","    losses = 0\n","    for i, batch in enumerate(train_iter):\n","        src = Variable(batch['embedding'], requires_grad=False).to(device)\n","        trg = Variable(batch['poses'],requires_grad=False).to(device)\n","        #begin_frame = torch.zeros((trg.shape[0],1,trg.shape[2])).cuda()\n","        begin_frame = src.unsqueeze(1).to(device)\n","        trg = torch.cat((begin_frame, trg),axis=1)\n","        tgt_mask = Variable(subsequent_mask(max_sequence_length + 1),requires_grad=False).to(device)\n","        padded_mask = (trg[:,:,-counter_index] != pad).unsqueeze(-2)\n","        other_mask = (padded_mask & tgt_mask).to(device)\n","        # todo check the way parameters are passed as expected\n","        # out = model.forward(src, trg[:, :-1], trg_mask[:, :-1, :-1])\n","        # TRY : changing the dimension of noise and accordingly change the model\n","        noise = Variable(torch.randn((src.shape[0], src.shape[1])).cuda())\n","        src = torch.cat((src,noise),1)\n","        out = model.forward(src, trg, other_mask)\n","        # padded_mask = padded_mask.squeeze(1).unsqueeze(2)\n","        # padded_mask = tile(padded_mask,2,102)\n","\n","        # padding = torch.zeros_like(trg)\n","        # # change out to only include loss from real frames\n","        # out = torch.where(padded_mask,out,padding)\n","  \n","        # out = torch.where(out[:,:,360:361]<=1, out, torch.zeros_like(out))\n","        # loss_counter = criterion(out[:,:,360:361], trg[:,:,360:361])\n","        # loss_pose = criterion(out[:,:,:360:], trg[:,:,:360:]) \n","        # total_loss = 0.05 * loss_counter +  loss_pose\n","        total_loss = criterion(out[:,:-1], trg[:,1::])\n","        losses += total_loss\n","\n","        # torso_loss = criterion(out[:,:,:16:], trg[:,:,:16:])\n","        # finger_loss = criterion(out[:,:,16::], trg[:,:,16::])\n","        # total_loss = torso_loss + 10 * finger_loss\n","\n","        total_loss.backward()\n","        model_opt.step()\n","        model_opt.optimizer.zero_grad()\n","        if i % 10 == 1:\n","            print(i, epoch, total_loss, model_opt._rate)\n","        if (i % 100 == 0 and epoch % 100 == 0):\n","            trim_frames(out[1],trg[1], 'output_movies/mov_%d_%d.mp4'%(epoch, i), 'output_movies/gt_mov_%d_%d.mp4'%(epoch, i) )\n","    losses_list.append(losses / batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4kFYs7nUgRb"},"source":["def valid_epoch_transformer(valid_iter, model, criterion):\n","    model.eval()\n","    total_loss = 0\n","    total_words = 0\n","    with torch.no_grad():  \n","      for i, batch in enumerate(valid_iter):\n","        src = Variable(batch['embedding'], requires_grad=False).to(device)\n","        trg = Variable(batch['poses'],requires_grad=False).to(device)\n","        begin_frame = src.unsqueeze(1).to(device)\n","        trg = torch.cat((begin_frame, trg),axis=1)\n","        tgt_mask = Variable(subsequent_mask(max_sequence_length + 1),requires_grad=False).to(device)\n","        padded_mask = (trg[:,:,-2] != pad).unsqueeze(-2)\n","        other_mask = (padded_mask & tgt_mask).to(device)\n","        noise = Variable(torch.randn((src.shape[0], src.shape[1])).cuda())\n","        src = torch.cat((src,noise),1)\n","        out = model.forward(src, trg, other_mask)\n","        total_loss += criterion(out[:,:-1], trg[:,1::])\n","        total_words += trg.shape[0]\n","    return  total_loss / total_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFSI-6NZuW7-"},"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","import matplotlib.animation as animation\n","\n","def trim_frames(out,trg,v_name1,v_name2):\n","    out = out.detach().float()\n","    trg = trg.detach().float()\n","    padded_mask = (trg[:,-counter_index] != pad).unsqueeze(1).cuda()\n","    padded_mask = tile(padded_mask,1,trg.shape[-1])\n","    padding = torch.fill_(torch.zeros_like(trg),pad).float()\n","    # change out to only include loss from real frames\n","    out = torch.where(padded_mask,out,padding)\n","    trg = torch.where(padded_mask,trg,padding)\n","    get_video(out,v_name1)\n","    get_video(trg,v_name2)\n","\n","def get_video(video, video_name):\n","    video = video.cpu().detach()\n","    # total=[]\n","    # video = sorted(video, key=lambda v: v[100]) #sort by predicted counter value (element at index 360)\n","    print(max([v[-counter_index] for v in video]))\n","    total = video\n","    # for output in video:\n","    #   o = []\n","    #   for i in range(0,len(output)-2,2):\n","    #     o.append(output.data[i])\n","    #     o.append(output.data[i+1])\n","    #   total.append(o)\n","\n","    fig = plt.figure(figsize=(8, 6),dpi = 90)\n","    ax  = fig.add_subplot(111)\n","\n","    def drawLine(i,j,pose,ax):\n","      x_points, y_points = [pose[i*2],pose[j*2]],[-pose[i*2+1].item(),-pose[j*2+1].item()]\n","      if (x_points[0] == pad and x_points[1] == pad and y_points[0] == pad and y_points[1] == pad ):\n","        return\n","      ax.plot(x_points, y_points, color = 'b')\n","\n","    lines4disp = ((0,1),(1,2),(2,3),(3,4),(1,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11),(11,12),(8,13),(13,14),(14,15),(15,16),(8,17),(7,18),(18,19),(19,20),(8,21),(21,22),(22,23),(23,24),(8,25),(25,26),(26,27,(27,28),(4,29),(29,30),(30,31),(31,32),(32,33),29,34),(34,35),(35,36),(36,37),(29,38),(38,39),(39,40),(40,41),(29,42),(42,43),(43,44),(44,45),(29,46),(46,47),(47,48),(48,49))\n","\n","    def animate2(i):\n","      ax.clear()\n","      for s in lines4disp:\n","        drawLine(s[0],s[1],np.array(total[i]),ax)\n","      ax.scatter(np.array(total[i][100:-counter_index:2]), -1 * np.array(total[i][101:-counter_index:2]))\n","\n","    # create animation\n","    ani = animation.FuncAnimation(fig, animate2, frames=range(1, len(total)), interval=100, repeat=False)\n","    ani.save(video_name)\n","\n","    # start animation\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85E9X5dDWe0V"},"source":["def tile(a, dim, n_tile):\n","    init_dim = a.size(dim)\n","    repeat_idx = [1] * a.dim()\n","    repeat_idx[dim] = n_tile\n","    a = a.repeat(*(repeat_idx))\n","    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).cuda()\n","    return torch.index_select(a, dim, order_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LC-LmRYf1jHR"},"source":["def plot_losses(loss_list, plot_name = \"Transformer losses\"):\n","  ax1 = plt.subplot(111)\n","  ax1.plot(loss_list)\n","  ax1.title.set_text(plot_name)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIS9fGZp24nt"},"source":["losses_list = []\n","criterion = nn.MSELoss()\n","# TRY to change input dimesion and pass that dimension of noise accordingly\n","model = make_model(d_model_input=500, d_model_output=250).to(device)\n","model_opt = get_std_opt(model)\n","validation_losses = []\n","for epoch in range(2000):\n","    train_epoch(train_loader, model, criterion, model_opt, epoch)\n","    validation_losses.append(valid_epoch_transformer(validation_loader, model, criterion))\n","    if (epoch+1) % 100 == 0:\n","      plot_losses(validation_losses,\"Validation_Losses\")\n","with open(\"Validation Losses for Transformer.txt\",\"w\") as f:\n","  f.write(validation_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iodm04rbRkV4"},"source":["# torch.save(model, 'trained_transformer_204.pkl')\n","# model_path = 'trained_transformer_204.pkl'\n","# model = torch.load(model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct8U3maY0joC"},"source":["def draw_pose(output):\n","  # we need to remove the third coordinate from each point\n","  o = []\n","  t = []\n","  for i in range(0,len(output)-counter_index,2):\n","    o.append(output.data[i])\n","    o.append(output.data[i+1])\n","  fig = plt.figure(figsize=(8, 6),dpi = 90)\n","  ax  = fig.add_subplot(111)\n","  def drawLine(i,j,pose,ax):\n","    x_points, y_points = [pose[i*2],pose[j*2]],[-pose[i*2+1],-pose[j*2+1]]\n","    ax.plot(x_points, y_points, color = 'b')\n","\n","  # structure = skeletalModel.getSkeletalModelStructure()\n","  lines4disp = ((0,1),(1,2),(2,3),(3,4),(1,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11),(11,12),(8,13),(13,14),(14,15),(15,16),(8,17),(7,18),(18,19),(19,20),(8,21),(21,22),(22,23),(23,24),(8,25),(25,26),(26,27,(27,28),(4,29),(29,30),(30,31),(31,32),(32,33),29,34),(34,35),(35,36),(36,37),(29,38),(38,39),(39,40),(40,41),(29,42),(42,43),(43,44),(44,45),(29,46),(46,47),(47,48),(48,49))\n","\n","  for s in lines4disp:\n","    drawLine(s[0],s[1],o,ax)\n","  print(o[100::2])\n","  ax.scatter(np.array(o[100::2]), -1 * np.array(o[101::2]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1mDXF9M37ik"},"source":["draw_pose(dataset.__getitem__(0)['poses'][3].cpu().detach())\n","get_video(dataset.__getitem__(0)['poses'], \"./src/Bhagyashree Experiments/output_movies/video_with_faces.mp4\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HEL-lyg7X5P"},"source":["!ls src"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTpL8D1wqA2l"},"source":["data = json.load(open(os.path.join('data/interim/Training Data/2D Data/','01576.json')))\n","max_count = max([int(i) for i in list(data.keys())])\n","actual_count = 0\n","temp = []\n","for count in data.keys():\n","  if len(data[count]['people']) > 0:\n","      t = data[count]['people'][0]['pose_keypoints_2d'][:24:] + data[count]['people'][0]['hand_left_keypoints_2d'] + data[count]['people'][0]['hand_right_keypoints_2d'] + [actual_count / max_count] + [0]\n","      temp.append(t)\n","      actual_count += 1\n","temp = np.array(temp)\n","n = temp.shape[-1] - 2\n","X = temp[:, 0:n:3]\n","Y = temp[:, 1:n:3]\n","W = temp[:, 2:n:3]\n","# normalization\n","X, Y = normalization(X, Y)\n","# Delete all skeletal models which have a lot of missing parts.\n","X, Y, W = prune(X, Y, W, (0, 1, 2, 3, 4, 5, 6, 7), 0.3, \"float32\")\n","# Preliminary filtering: weighted linear interpolation of missing points.\n","X, Y, W = interpolation(X, Y, W, 0.99, \"float32\")\n","arr = np.append(convList2Array([X, Y]),temp[:,-2:], axis=1)\n","padded_input = np.zeros((max_sequence_length - len(arr), len(arr[0])))\n","padded_input[:, -2] = pad\n","arr = np.vstack((arr, padded_input))\n","print(arr.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hpv8adf2OmdP"},"source":["# arr = torch.tensor(arr)\n","# target = torch.zeros(1, 185, 102)\n","# source = torch.ones(1, arr.shape[0], 102)\n","# target[:, 0:arr.shape[0], :] = source\n","encoder_input = torch.tensor(embeddings_dict[\"agree\"], dtype=torch.float32).unsqueeze(0)\n","decoder_input = torch.zeros(185,102).unsqueeze(0)\n","noise = Variable(torch.randn((encoder_input.shape[0], encoder_input.shape[1])))\n","decoder_input[0][0] = encoder_input #arr[0]\n","encoder_input = torch.cat((encoder_input,noise),1)\n","# torch.zeros((185,102)).unsqueeze(0)\n","\n","i = 0\n","model.eval()\n","other_mask = (torch.zeros(1,1,185) != 0).cuda()\n","with torch.no_grad():\n","    while ( i <= 183):\n","        src = Variable(encoder_input, requires_grad=False).cuda()\n","        trg = Variable(decoder_input,requires_grad=False).cuda()\n","        other_mask[0][0][i] = True\n","        out = model.forward(src, trg, other_mask)\n","        decoder_input[0,i+1,:] = out[0,i,:]\n","        i += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_rzJwnWp1BP"},"source":["# get_video(torch.tensor(arr), 'output_movies_b/adult_true.mp4')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzPSD0_JiS7C"},"source":["# get_video(torch.tensor(decoder_input[0]), 'output_movies_b/all_out.mp4')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WueK3RXhffxK"},"source":["trim_frames(decoder_input[0].cuda(), torch.from_numpy(arr).cuda(), 'output_movies/ag_out_1.mp4','output_movies/ag_gt.mp4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IvSByM7vqBPN"},"source":["GAN"]},{"cell_type":"code","metadata":{"id":"2lGCgl1AqC-v"},"source":["class Generator(nn.Module):\n","    # initializers\n","    def __init__(self, pretrained_model, d=128):\n","        super(Generator, self).__init__()\n","        self.model = pretrained_model\n","        self.model_opt = get_std_opt(self.model)\n","\n","    # forward method\n","    def forward(self, src, trg = None):\n","        if trg is not None:\n","          begin_frame = src[:,:embedding_dim:].unsqueeze(1).to(device)\n","          trg = torch.cat((begin_frame, trg),axis=1)\n","          tgt_mask = Variable(subsequent_mask(max_sequence_length + 1),requires_grad=False).to(device)\n","          padded_mask = (trg[:,:,-2] != pad).unsqueeze(-2)\n","          other_mask = (padded_mask & tgt_mask).to(device)\n","          # todo check the way parameters are passed as expected\n","          # out = model.forward(src, trg[:, :-1], trg_mask[:, :-1, :-1])\n","          out = self.model.forward(src, trg, other_mask)\n","          return out\n","        else:\n","          # this is for inference\n","          trg = torch.zeros(1,max_sequence_length+1,pose_dim)\n","          trg[0][0] = src[0][:embedding_dim:] #arr[0]\n","          other_mask = (torch.zeros(1,1,max_sequence_length+1) != 0).cuda()\n","          i = 0\n","          with torch.no_grad():\n","              while ( i <= max_sequence_length - 1):\n","                  src = Variable(src, requires_grad=False).cuda()\n","                  trg = Variable(trg,requires_grad=False).cuda()\n","                  other_mask[0][0][i] = True\n","                  out = model.forward(src, trg, other_mask)\n","                  trg[0,i+1,:] = out[0,i,:]\n","                  i += 1\n","          return trg[0]  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kd7R7nWWqDOZ"},"source":["class Print(nn.Module):\n","  def __init__(self):\n","    super(Print, self).__init__()\n","\n","  def forward(self,x):\n","    print('print',x.shape)\n","    return x\n","    \n","class Discriminator(nn.Module):\n","    def __init__(self, d=64):\n","        super(Discriminator, self).__init__()\n","        self.discriminate = nn.Sequential(\n","            # TRY Removing batch norm or stacking more conv1D\n","            # check input dim\n","            nn.Conv1d(186, d, 10),\n","            # Print(),\n","            # nn.Conv1d(370, d, 10),            \n","            nn.LeakyReLU(),\n","            nn.Conv1d(d, d, 10),\n","            nn.BatchNorm1d(d),\n","            nn.LeakyReLU(),\n","            nn.Conv1d(d, 1, 10),\n","            nn.Linear(75, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input, word):\n","        # print(input.shape, word.shape)\n","        input = torch.cat((input,word), 1)\n","        x = self.discriminate(input)\n","        return x.squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clx8U07zqDK4"},"source":["adversarial_loss = torch.nn.BCELoss()\n","regression_loss = torch.nn.MSELoss()\n","def weight_initialization(m):\n","    # custom weights initialization for both networks\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.xavier_uniform(m.weight.data)\n","    # elif classname.find('BatchNorm') != -1:\n","    #     nn.init.xavier_uniform(m.weight.data)\n","    #     nn.init.xavier_uniform(m.bias.data)\n","# Initialize Generator and discriminator\n","\n","# we will pass pre trained transformer model\n","# model = make_model(N=6, d_model_input=204, d_model_output=102, d_ff=1024, h=6, dropout=0.1)\n","# model = torch.load(\"\")\n","# TRY : change model and try pretrained or try creating a new transformer\n","generator = Generator(model)\n","discriminator = Discriminator()\n","\n","if torch.cuda.is_available():\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","\n","# Initialize weights\n","generator.apply(weight_initialization)\n","discriminator.apply(weight_initialization)\n","\n","lr = 1e-3\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr) #betas=(args.beta1, args.beta2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr) #betas=(args.beta1, args.beta2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5qwqqXNBiYd"},"source":["def valid_epoch_GAN(valid_iter, generator, discriminator, criterion):\n","    model.eval()\n","    total_generative_loss = 0\n","    total_words = 0\n","    with torch.no_grad():  \n","      for i, batch in enumerate(valid_iter):\n","        embeddings = batch['embedding'].cuda()\n","        poses = batch['poses'].cuda()\n","        real_imgs = Variable(poses.type(torch.FloatTensor).cuda())\n","        \n","        valid = Variable(torch.ones(real_imgs.shape[0]).cuda(), requires_grad=False)\n","        \n","        noise = Variable(torch.randn((real_imgs.shape[0], output_dim)).cuda())\n","        gen_input = torch.cat((embeddings,noise),1)\n","        gen_imgs = generator(gen_input, poses)\n","        gen_y_for_D = tile(embeddings.cuda().unsqueeze(1), 1, max_sequence_length)\n","\n","        g_loss_regression = 100 * regression_loss(gen_imgs[:,:-1:], poses)\n","        g_loss_adversarial = 0.001 * adversarial_loss(discriminator(gen_imgs[:,1::],gen_y_for_D).squeeze(), valid)\n","        total_generative_loss += g_loss_regression + g_loss_adversarial\n","        total_words += embeddings.shape[0]\n","\n","    return  total_generative_loss / total_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQ-Vwm7xqcTC"},"source":["batch_size = 16\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","n_epochs = 5\n","output_dim = 102\n","sample_interval = 500\n","input_dim = 204\n","pose_dim = 102\n","embedding_dim = 102\n","generator_losses = []\n","discriminator_losses = [] \n","generator_validation_losses = []\n","for epoch in range(n_epochs):\n","    generator_loss = 0\n","    discriminator_loss = 0\n","    for i, batch in enumerate(train_loader):\n","        embeddings = batch['embedding'].cuda()\n","        poses = batch['poses'].cuda()\n","        \n","        # Configure input\n","        real_imgs = Variable(poses.type(torch.FloatTensor).cuda())\n","        real_y = torch.ones(real_imgs.shape[0],output_dim).cuda()\n","        # real_y = real_y.scatter_(1, labels.view(Batch_Size, 1), 1).view(Batch_Size, N_Class, 1, 1).contiguous()\n","        real_y = Variable(real_y.cuda())\n","\n","        \n","        valid = Variable(torch.ones(real_imgs.shape[0]).cuda(), requires_grad=False)\n","        fake = Variable(torch.zeros(real_imgs.shape[0]).cuda(), requires_grad=False)\n","\n","        # Sample noise and labels as generator input\n","        # noise = Variable(torch.randn((Batch_Size, output_dim)).cuda())\n","        # gen_labels = (torch.rand(Batch_Size, 1) * N_Class).type(torch.LongTensor)\n","        # check 1 dim\n","        # gen_y = Variable(gen_y.scatter_(1, gen_labels.view(Batch_Size, 1), 1).view(Batch_Size, N_Class,1,1).cuda())\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","        optimizer_D.zero_grad()\n","        # Loss for real images\n","        # gen_y = tile(gen_y.unsqueeze(2), 2, max_sequence_length).reshape(-1, 185, 364)\n","        # print(gen_y.unsqueeze(1).shape, real_imgs.shape)\n","\n","        # gen_y = tile(embeddings.cuda().unsqueeze(1), 1, max_sequence_length)\n","        gen_y = embeddings.unsqueeze(1)\n","        # print(gen_y.shape)\n","        d_real_loss = adversarial_loss(discriminator(real_imgs, gen_y), valid) #batch x 185 x 364; batch x 300\n","        # Loss for fake images\n","        # TRY change noise dimension according to the pretrained or according to what is passed in make model\n","        noise = Variable(torch.randn((real_imgs.shape[0], output_dim)).cuda())\n","        gen_input = torch.cat((embeddings,noise),1)\n","        gen_imgs = generator(gen_input, poses)\n","        # gen_y_for_D = tile(embeddings.cuda().unsqueeze(1), 1, max_sequence_length)\n","        gen_y_for_D = embeddings.unsqueeze(1)\n","        # gen_y_for_D = gen_y.view(Batch_Size, N_Class, 1, 1).contiguous().expand(-1, -1, img_size, img_size)\n","        d_fake_loss = adversarial_loss(discriminator(gen_imgs.detach()[:,1:],gen_y_for_D), fake)\n","        # Total discriminator loss\n","        d_loss = (d_real_loss + d_fake_loss)\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","        # TRY different weights for regression loss weights\n","        g_loss_regression = 100 * regression_loss(gen_imgs[:,:-1:], poses)\n","        g_loss_adversarial = 0.001 * adversarial_loss(discriminator(gen_imgs[:,1::],gen_y_for_D).squeeze(), valid)\n","        # total_genereator_loss = 100 * g_loss_regression + 0.001 * g_loss_adversarial\n","        total_generator_loss = g_loss_regression + g_loss_adversarial\n","        total_generator_loss.backward()\n","        optimizer_G.step()\n","\n","        generator_loss += total_generator_loss\n","        discriminator_loss += d_loss\n","\n","        batches_done = epoch * len(train_loader) + i\n","        if batches_done % sample_interval == 0:\n","            # fixed labels\n","            sample_embedding = embeddings[0:1]\n","            sample_poses = poses[0:1]\n","            noise = Variable(torch.randn((sample_embedding.shape[0], output_dim)).cuda())\n","            gen_input = torch.cat((sample_embedding,noise),1)\n","            gen_imgs = generator(gen_input)\n","            # save_image(gen_imgs.data, img_save_path + '/%d-%d.png' % (epoch,batches_done), nrow=N_Class, normalize=True)\n","            trim_frames(gen_imgs[:-1:],sample_poses[0],\"src/Bhagyashree Experiments/output_movies/gen_output\"+str(epoch)+\".mp4\",\"src/Bhagyashree Experiments/output_movies/gen_gt\"+str(epoch)+\".mp4\")\n","    \n","    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs, i, len(train_loader), d_loss.data.cpu(), total_generator_loss.data.cpu()))\n","    discriminator_losses.append(discriminator_loss / batch_size)\n","    generator_losses.append(generator_loss/batch_size)\n","    generator_validation_losses.append(valid_epoch_GAN(validation_loader, generator, discriminator, regression_loss))\n","\n","# plotting the losses\n","print(generator_losses,discriminator_losses)\n","plot_losses(generator_losses,\"Generator Loss\")\n","with open(\"Generator Loss.txt\",\"w\") as f:\n","  f.write(generator_losses)\n","plot_losses(discriminator_losses,\"Discriminator Loss\")\n","with open(\"Discriminator Loss.txt\",\"w\") as f:\n","  f.write(discriminator_losses)\n","plot_losses(generator_validation_losses,\"Generator Validation Loss\")\n","with open(\"Generator Validation Loss.txt\",\"w\") as f:\n","  f.write(generator_validation_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5S2BcIoOUgRg"},"source":["# A Real World Example"]},{"cell_type":"code","metadata":{"id":"F8MTIJTWUgRl"},"source":["# Detail. Batching seems to matter quite a bit. \n","# This is temporary code for dynamic batching based on number of tokens.\n","# This code should all go away once things get merged in this library.\n","#Todo if not necessary remove later\n","BATCH_SIZE = 4096\n","global max_src_in_batch, max_tgt_in_batch\n","def batch_size_fn(new, count, sofar):\n","    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n","    global max_src_in_batch, max_tgt_in_batch\n","    if count == 1:\n","        max_src_in_batch = 0\n","        max_tgt_in_batch = 0\n","    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n","    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n","    src_elements = count * max_src_in_batch\n","    tgt_elements = count * max_tgt_in_batch\n","    return max(src_elements, tgt_elements)\n","\n","class MyIterator(data.Iterator):\n","    def create_batches(self):\n","        if self.train:\n","            def pool(d, random_shuffler):\n","                for p in data.batch(d, self.batch_size * 100):\n","                    p_batch = data.batch(\n","                        sorted(p, key=self.sort_key),\n","                        self.batch_size, self.batch_size_fn)\n","                    for b in random_shuffler(list(p_batch)):\n","                        yield b\n","            self.batches = pool(self.data(), self.random_shuffler)\n","            \n","        else:\n","            self.batches = []\n","            for b in data.batch(self.data(), self.batch_size,\n","                                          self.batch_size_fn):\n","                self.batches.append(sorted(b, key=self.sort_key))\n","\n","def rebatch(pad_idx, batch):\n","    \"Fix order in torchtext to match ours\"\n","    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n","    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n","    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n","\n","train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n","                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n","                        batch_size_fn=batch_size_fn, train=True)\n","valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n","                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n","                        batch_size_fn=batch_size_fn, train=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wamR3SPdUgRo"},"source":["# Create the model an load it onto our GPU.\n","model = make_model()\n","model_opt = get_std_opt(model)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SStCCZoiUgRp"},"source":["\n","criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n","criterion.cuda()\n","for epoch in range(15):\n","    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n","    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NO9lsw2UgRt"},"source":["\n","OTHER"]},{"cell_type":"code","metadata":{"id":"B8BVm-hEUgRw"},"source":["BOS_WORD = '<s>'\n","EOS_WORD = '</s>'\n","BLANK_WORD = \"<blank>\"\n","SRC = data.Field()\n","TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n","\n","MAX_LEN = 100\n","train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n","                                    exts=('.en', '.fr'),\n","                                    fields=(SRC, TGT), \n","                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n","                                         len(vars(x)['trg']) <= MAX_LEN)\n","SRC.build_vocab(train.src, max_size=50000)\n","TGT.build_vocab(train.trg, max_size=50000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFdZyOIzUgRx"},"source":["pad_idx = TGT.vocab.stoi[\"<blank>\"]\n","print(pad_idx)\n","model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n","model_opt = get_opt(model)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cinuTkbtUgRz"},"source":["criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n","criterion.cuda()\n","for epoch in range(15):\n","    train_epoch(train_iter, model, criterion, model_opt)\n","    valid_epoch()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsoeJn4bUgR1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LadFBIEUgR3"},"source":["print(pad_idx)\n","print(len(SRC.vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kP_Au0bHUgR7"},"source":["torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqKKIhoOUgR-"},"source":["#weight = torch.ones(len(TGT.vocab))\n","#weight[pad_idx] = 0\n","#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n","criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n","criterion.cuda()\n","for epoch in range(15):\n","    train_epoch(train_iter, model, criterion, model_opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35JC6i9QUgSB"},"source":["1 10.825187489390373 6.987712429686844e-07\n","101 9.447168171405792 3.56373333914029e-05\n","201 7.142856806516647 7.057589553983712e-05\n","301 6.237934365868568 0.00010551445768827134\n","401 5.762486848048866 0.00014045301983670557\n","501 5.415792358107865 0.00017539158198513977\n","601 5.081815680023283 0.000210330144133574\n","701 4.788327748770826 0.00024526870628200823\n","801 4.381739928154275 0.0002802072684304424\n","901 4.55433791608084 0.00031514583057887664\n","1001 4.911875109748507 0.0003500843927273108\n","1101 4.0579032292589545 0.0003850229548757451\n","1201 4.2276234351193125 0.0004199615170241793\n","1301 3.932735869428143 0.00045490007917261356\n","1401 3.8179439397063106 0.0004898386413210477\n","1501 3.3608515430241823 0.000524777203469482\n","1601 3.832796103321016 0.0005597157656179162\n","1701 2.907085266895592 0.0005946543277663504\n","1801 3.5280659823838505 0.0006295928899147847\n","1901 2.895841649500653 0.0006645314520632189\n","2001 3.273784235585481 0.000699470014211653\n","2101 3.181488689899197 0.0007344085763600873\n","2201 3.4151616653980454 0.0007693471385085215\n","2301 3.4343731447588652 0.0008042857006569557\n","2401 3.0505455391539726 0.0008392242628053899\n","2501 2.8089329147478566 0.0008741628249538242\n","2601 2.7827929875456903 0.0009091013871022583\n","2701 2.4428516102489084 0.0009440399492506926\n","2801 2.4015486147254705 0.0009789785113991267\n","2901 2.3568112018401735 0.001013917073547561\n","3001 2.6349758653668687 0.0010488556356959952\n","3101 2.5981983028614195 0.0010837941978444295\n","3201 2.666826274838968 0.0011187327599928637\n","3301 3.0092043554177508 0.0011536713221412978\n","3401 2.4580375660589198 0.0011886098842897321\n","3501 2.586465588421561 0.0012235484464381662\n","3601 2.5663993963389657 0.0012584870085866006\n","3701 2.9430236657499336 0.0012934255707350347\n","3801 2.464644919440616 0.001328364132883469\n","3901 2.7124062888276512 0.0013633026950319032\n","4001 2.646443709731102 0.0013971932312809247\n","4101 2.7294750874862075 0.001380057517579748\n","4201 2.1295202329056337 0.0013635372009002666\n","4301 2.596563663915731 0.001347596306985731\n","4401 2.1265982036820787 0.0013322017384983986\n","4501 2.3880532500334084 0.0013173229858148\n","4601 2.6129120760888327 0.0013029318725783852\n","4701 2.2873719420749694 0.001289002331178292\n","4801 2.4949760700110346 0.0012755102040816328\n","4901 2.496607314562425 0.001262433067573089\n","5001 2.1889712483389303 0.0012497500749750088\n","5101 1.8677761815488338 0.0012374418168536253\n","5201 2.2992054556962103 0.0012254901960784316\n","5301 2.664361578106707 0.0012138783159049418\n","5401 2.705850490485318 0.0012025903795063202\n","5501 2.581445264921058 0.0011916115995949978\n","5601 2.2480602325085783 0.0011809281169581616\n","5701 1.9289666265249252 0.0011705269268863989\n","5801 2.4863578918157145 0.0011603958126073107\n","5901 2.632946971571073 0.0011505232849492607\n","6001 2.496141305891797 0.0011408985275576757\n","6101 2.6422974687084206 0.0011315113470699342\n","6201 2.448802186456305 0.0011223521277270118"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fe1Lp7XJxzSw"},"source":["temp = torch.Tensor([[[1,1,0,-1],[-1,-1,2,3],[1,0,2,3]],\n","                     [[1,-1,2,3],[-1,1,2,3],[0,0,2,3]],\n","                     [[2,-2,2,3],[-2,2,2,3],[0,2,2,3]],\n","                     [[2,0,2,3],[-2,0,2,3],[0,-2,2,3]]\n","                     ])\n","extra = torch.Tensor([[[1,1,1,1]],\n","                     [[1,1,1,1]],\n","                     [[1,1,1,1]],\n","                     [[1,1,1,1]]\n","                     ])\n","print(temp.size())\n","print(temp)\n","t = temp.repeat(1,5)\n","print(t.size())\n","print(t)\n","'''\n","temp = torch.cat((temp, extra),axis = 1)\n","print(temp)\n","#u = torch.where(temp[:,:,1:2]>=1, temp, torch.zeros_like(temp))\n","#print(u)\n","x_centers = torch.unsqueeze(temp[:,:,0],axis = 2)\n","y_centers = torch.unsqueeze(temp[:,:,1],axis = 2)\n","#print(x_centers,y_centers)\n","centers = torch.cat((x_centers,y_centers), 2).repeat(repeats=(1,1,temp.size()[2]-2))\n","centers[:,:, -1::] = 0\n","print(centers)\n","temp -= centers\n","print(temp)   \n","# indices_of_one = torch.where(temp[:,:,1] >= 1.0,temp,torch.zeros(3,2))\n","# print(indices_of_one)\n","# temp = torch.where(temp <= 1, temp, torch.zeros(4,3,2))\n","# print(temp)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvzQRiZ8ZeFB"},"source":["real_y = torch.zeros(16, 10)\n","real_y = real_y.scatter_(1, labels.view(Batch_Size, 1), 1).view(Batch_Size, N_Class, 1, 1).contiguous()\n","real_y = Variable(real_y.expand(-1, -1, img_size, img_size).cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElTDB2fcLXCx"},"source":["def load_embeddings():\n","    word_embeddings = []\n","    words = []\n","    with (open(\"data/interim/embeddings.pkl\", \"rb\")) as openfile:\n","        while True:\n","            try:\n","                word_embeddings.append(pickle.load(openfile))\n","            except EOFError:\n","                break\n","    with (open(\"./data/interim/words.pkl\", \"rb\")) as openfile:\n","        while True:\n","            try:\n","                words.append(pickle.load(openfile))\n","            except EOFError:\n","                break\n","    return word_embeddings, words\n","\n","def get_word_embedding(word, word_embeddings, words):\n","    indices = np.argwhere(words[0] == word)[0]\n","    return word_embeddings[0][indices[0]]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL2AVW8uLc9l"},"source":["encoder_input = torch.tensor(embeddings_dict['all'], dtype=torch.float32).unsqueeze(0)\n","decoder_input = torch.zeros(185,102).unsqueeze(0)\n","# torch.zeros((185,102)).unsqueeze(0)\n","\n","i = 0\n","model.eval()\n","other_mask = (torch.zeros(1,1,185) != 0).cuda()\n","with torch.no_grad():\n","    while ( i <= 183):\n","        src = Variable(encoder_input, requires_grad=False).cuda()\n","        trg = Variable(decoder_input,requires_grad=False).cuda()\n","        other_mask[0][0][i] = True\n","        out = model.forward(src, trg, other_mask)\n","        decoder_input[0,i+1,:] = out[0,i,:]\n","        i += 1\n","        # print(i)"],"execution_count":null,"outputs":[]}]}